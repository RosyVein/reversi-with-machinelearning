{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s2_1zKlBhaVO",
        "outputId": "332d7b34-b26a-4555-83d0-0c0b69fe07a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting creversi\n",
            "  Downloading creversi-0.0.1-cp310-cp310-manylinux_2_24_x86_64.whl (711 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m711.0/711.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: creversi\n",
            "Successfully installed creversi-0.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install creversi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XvIxe8ZWK_9r",
        "outputId": "cd6c4ecc-2fad-41fb-deb1-314d5316429e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/creversi/__init__.py:1: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  from .creversi import *\n"
          ]
        }
      ],
      "source": [
        "import gym\n",
        "import creversi.gym_reversi\n",
        "from creversi import *\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import datetime\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import namedtuple\n",
        "from itertools import count\n",
        "import logging\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from IPython.display import display, SVG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJl6Yit6hVcX",
        "outputId": "54c38a92-c9a9-41cd-896e-2ad677baf878"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:31: UserWarning: \u001b[33mWARN: A Box observation space has an unconventional shape (neither an image, nor a 1D vector). We recommend flattening the observation to have only a 1D vector or use a custom policy to properly process the data. Actual observation shape: (8, 8)\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-12-23 03:58:52 DEBUG Initializing MLIR with module: _site_initialize_0\n",
            "2023-12-23 03:58:52 DEBUG Registering dialects from initializer <module 'jaxlib.mlir._mlir_libs._site_initialize_0' from '/usr/local/lib/python3.10/dist-packages/jaxlib/mlir/_mlir_libs/_site_initialize_0.so'>\n",
            "2023-12-23 03:58:52 DEBUG etils.epath found. Using etils.epath for file I/O.\n"
          ]
        }
      ],
      "source": [
        "resume = ''\n",
        "logging.basicConfig(format='%(asctime)s %(levelname)s %(message)s', datefmt='%Y-%m-%d %H:%M:%S', stream=sys.stdout, level=logging.DEBUG, force=True)\n",
        "\n",
        "env = gym.make('Reversi-v0').unwrapped\n",
        "\n",
        "# if gpu is to be used\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "######################################################################\n",
        "# Replay Memory\n",
        "\n",
        "Transition = namedtuple('Transition',\n",
        "                        ('state', 'action', 'next_state', 'next_actions', 'reward'))\n",
        "\n",
        "\n",
        "class ReplayMemory(object):\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.memory = []\n",
        "        self.position = 0\n",
        "\n",
        "    def push(self, *args):\n",
        "        \"\"\"Saves a transition.\"\"\"\n",
        "        if len(self.memory) < self.capacity:\n",
        "            self.memory.append(None)\n",
        "        self.memory[self.position] = Transition(*args)\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# DQN\n",
        "\n",
        "k = 192\n",
        "fcl_units = 256\n",
        "class DQN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(DQN, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(2, k, kernel_size=3, padding=1)\n",
        "    self.bn1 = nn.BatchNorm2d(k)\n",
        "    self.conv2 = nn.Conv2d(k, k, kernel_size=3, padding=1)\n",
        "    self.bn2 = nn.BatchNorm2d(k)\n",
        "    self.conv3 = nn.Conv2d(k, k, kernel_size=3, padding=1)\n",
        "    self.bn3 = nn.BatchNorm2d(k)\n",
        "    self.conv4 = nn.Conv2d(k, k, kernel_size=3, padding=1)\n",
        "    self.bn4 = nn.BatchNorm2d(k)\n",
        "    self.conv5 = nn.Conv2d(k, k, kernel_size=3, padding=1)\n",
        "    self.bn5 = nn.BatchNorm2d(k)\n",
        "    self.conv6 = nn.Conv2d(k, k, kernel_size=3, padding=1)\n",
        "    self.bn6 = nn.BatchNorm2d(k)\n",
        "    self.conv7 = nn.Conv2d(k, k, kernel_size=3, padding=1)\n",
        "    self.bn7 = nn.BatchNorm2d(k)\n",
        "    self.conv8 = nn.Conv2d(k, k, kernel_size=3, padding=1)\n",
        "    self.bn8 = nn.BatchNorm2d(k)\n",
        "    self.conv9 = nn.Conv2d(k, k, kernel_size=3, padding=1)\n",
        "    self.bn9 = nn.BatchNorm2d(k)\n",
        "    self.conv10 = nn.Conv2d(k, k, kernel_size=3, padding=1)\n",
        "    self.bn10 = nn.BatchNorm2d(k)\n",
        "    self.fcl1 = nn.Linear(k * 64, fcl_units)\n",
        "    self.fcl2 = nn.Linear(fcl_units, 65)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.bn1(self.conv1(x)))\n",
        "    x = F.relu(self.bn2(self.conv2(x)))\n",
        "    x = F.relu(self.bn3(self.conv3(x)))\n",
        "    x = F.relu(self.bn4(self.conv4(x)))\n",
        "    x = F.relu(self.bn5(self.conv5(x)))\n",
        "    x = F.relu(self.bn6(self.conv6(x)))\n",
        "    x = F.relu(self.bn7(self.conv7(x)))\n",
        "    x = F.relu(self.bn8(self.conv8(x)))\n",
        "    x = F.relu(self.bn9(self.conv9(x)))\n",
        "    x = F.relu(self.bn10(self.conv10(x)))\n",
        "    x = F.relu(self.fcl1(x.view(-1, k * 64)))\n",
        "    x = self.fcl2(x)\n",
        "    return x.tanh()\n",
        "\n",
        "def get_state(board):\n",
        "    features = np.empty((1, 2, 8, 8), dtype=np.float32)\n",
        "    board.piece_planes(features[0])\n",
        "    state = torch.from_numpy(features[:1]).to(device)\n",
        "    return state\n",
        "\n",
        "######################################################################\n",
        "# Training\n",
        "\n",
        "BATCH_SIZE = 256\n",
        "GAMMA = 0.99\n",
        "EPS_START = 0.9\n",
        "EPS_END = 0.05\n",
        "EPS_DECAY = 2000\n",
        "OPTIMIZE_PER_EPISODES = 16\n",
        "TARGET_UPDATE = 4\n",
        "\n",
        "policy_net = DQN().to(device)\n",
        "target_net = DQN().to(device)\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "target_net.eval()\n",
        "\n",
        "optimizer = optim.RMSprop(policy_net.parameters(), lr=1e-5)\n",
        "\n",
        "if resume:\n",
        "    print('resume {}'.format(resume))\n",
        "    checkpoint = torch.load(resume)\n",
        "    target_net.load_state_dict(checkpoint['state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "\n",
        "memory = ReplayMemory(131072)\n",
        "\n",
        "def epsilon_greedy(state, legal_moves):\n",
        "    sample = random.random()\n",
        "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
        "        math.exp(-1. * episodes_done / EPS_DECAY)\n",
        "\n",
        "    if sample > eps_threshold:\n",
        "        with torch.no_grad():\n",
        "            q = policy_net(state)\n",
        "            _, select = q[0, legal_moves].max(0)\n",
        "    else:\n",
        "        select = random.randrange(len(legal_moves))\n",
        "    return select\n",
        "\n",
        "temperature = 0.5\n",
        "def softmax(state, legal_moves):\n",
        "    with torch.no_grad():\n",
        "        q = policy_net(state)\n",
        "        log_prob = q[0, legal_moves] / temperature\n",
        "        select = torch.distributions.categorical.Categorical(logits=log_prob).sample()\n",
        "    return select\n",
        "\n",
        "def select_action(state, board):\n",
        "\n",
        "    legal_moves = list(board.legal_moves)\n",
        "\n",
        "    select = epsilon_greedy(state, legal_moves)\n",
        "    #select = softmax(state, board.legal_moves)\n",
        "\n",
        "    return legal_moves[select], torch.tensor([[legal_moves[select]]], device=device, dtype=torch.long)\n",
        "\n",
        "\n",
        "######################################################################\n",
        "# Training loop\n",
        "\n",
        "def optimize_model():\n",
        "    if len(memory) < BATCH_SIZE:\n",
        "        return\n",
        "    transitions = memory.sample(BATCH_SIZE)\n",
        "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
        "    # detailed explanation). This converts batch-array of Transitions\n",
        "    # to Transition of batch-arrays.\n",
        "    batch = Transition(*zip(*transitions))\n",
        "\n",
        "    # Compute a mask of non-final states and concatenate the batch elements\n",
        "    # (a final state would've been the one after which simulation ended)\n",
        "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
        "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
        "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
        "                                                if s is not None])\n",
        "    state_batch = torch.cat(batch.state)\n",
        "    action_batch = torch.cat(batch.action)\n",
        "    reward_batch = torch.cat(batch.reward)\n",
        "\n",
        "    # 合法手のみ\n",
        "    non_final_next_actions_list = []\n",
        "    for next_actions in batch.next_actions:\n",
        "        if next_actions is not None:\n",
        "            non_final_next_actions_list.append(next_actions + [next_actions[0]] * (30 - len(next_actions)))\n",
        "    non_final_next_actions = torch.tensor(non_final_next_actions_list, device=device, dtype=torch.long)\n",
        "\n",
        "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
        "    # columns of actions taken. These are the actions which would've been taken\n",
        "    # for each batch state according to policy_net\n",
        "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
        "\n",
        "    # Compute V(s_{t+1}) for all next states.\n",
        "    # Expected values of actions for non_final_next_states are computed based\n",
        "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
        "    # This is merged based on the mask, such that we'll have either the expected\n",
        "    # state value or 0 in case the state was final.\n",
        "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
        "    # 合法手のみの最大値\n",
        "    target_q = target_net(non_final_next_states)\n",
        "    # 相手番の価値のため反転する\n",
        "    next_state_values[non_final_mask] = -target_q.gather(1, non_final_next_actions).max(1)[0].detach()\n",
        "    # Compute the expected Q values\n",
        "    expected_state_action_values = next_state_values * GAMMA + reward_batch\n",
        "\n",
        "    # Compute Huber loss\n",
        "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
        "\n",
        "    logging.info(f\"{episodes_done}: loss = {loss.item()}\")\n",
        "\n",
        "    # Optimize the model\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    for param in policy_net.parameters():\n",
        "        param.grad.data.clamp_(-1, 1)\n",
        "    optimizer.step()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jaaeg16eMkGC"
      },
      "outputs": [],
      "source": [
        "######################################################################\n",
        "# main training loop\n",
        "\n",
        "episodes_done = 0\n",
        "for i_episode in range(12000):\n",
        "    # Initialize the environment and state\n",
        "    env.reset()\n",
        "    state = get_state(env.board)\n",
        "\n",
        "    for t in count():\n",
        "        # Select and perform an action\n",
        "        move, action = select_action(state, env.board)\n",
        "        next_board, reward, done, is_draw = env.step(move)\n",
        "\n",
        "        # todo: players\n",
        "\n",
        "        # display(SVG(env.board.to_svg(move)))\n",
        "        # print(move)\n",
        "\n",
        "\n",
        "        reward = torch.tensor([reward], device=device)\n",
        "        # print(reward)\n",
        "\n",
        "        # Observe new state\n",
        "        if not done:\n",
        "            next_state = get_state(next_board)\n",
        "            next_actions = list(next_board.legal_moves)\n",
        "        else:\n",
        "            next_state = None\n",
        "            next_actions = None\n",
        "\n",
        "        # Store the transition in memory\n",
        "        memory.push(state, action, next_state, next_actions, reward)\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "        # Move to the next state\n",
        "        state = next_state\n",
        "\n",
        "    episodes_done += 1\n",
        "\n",
        "    if i_episode % OPTIMIZE_PER_EPISODES == OPTIMIZE_PER_EPISODES - 1:\n",
        "        # Perform several episodes of the optimization (on the target network)\n",
        "        optimize_model()\n",
        "\n",
        "        # Update the target network, copying all weights and biases in DQN\n",
        "        if i_episode // OPTIMIZE_PER_EPISODES % TARGET_UPDATE == 0:\n",
        "            target_net.load_state_dict(policy_net.state_dict())\n",
        "\n",
        "model = 'epsilon_greedy_model.pt'\n",
        "print('save {}'.format(model))\n",
        "torch.save({'state_dict': target_net.state_dict(), 'optimizer': optimizer.state_dict()}, model)\n",
        "\n",
        "print('Complete')\n",
        "env.close()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
